{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b8e9d5-766a-47b7-8b0b-f13b6147c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "mongouser = os.getenv('MONGO_INITDB_ROOT_USERNAME')\n",
    "mongopass = os.getenv('MONGO_INITDB_ROOT_PASSWORD')\n",
    "\n",
    "client = MongoClient(f\"mongodb://{mongouser}:{mongopass}@mongodb:27017\")\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    " \n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1669569d-cc66-4ac2-8424-449906c61729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /pipeline_datalake is our mounted datalake volume with local machine\n",
    "terms = pd.read_excel('/pipeline_datalake/List of clinical definitions lookups.xlsx')\n",
    "my_terms_list = terms['Concept Name'].to_list()\n",
    "lower_terms = [c.lower() for c in my_terms_list]\n",
    "# This was run once, don't need to run again\n",
    "run_annotation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af113f40-ea1f-48a9-8155-d07c5d2640b9",
   "metadata": {},
   "source": [
    "## Purpose of this notebook is just to prep 20 examples for me to annotate\n",
    "\n",
    "The output is two files for use in modeling in the next two notebooks which will be at the base of the repo\n",
    "\n",
    "- Annoted terms.xlsx -> has the labels created off a rough estimation of entity types\n",
    "- Annoted terms With Set.xlsx' -> me taking the output and using UMLS browser, athena web browser from ohdsi, google, and gemini to validate those 20\n",
    "- Lessons learned were key to certain decisions and clarity around what is correct\n",
    "- The next notebook (2) will build a default prompt model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7be90-6411-4517-bac7-14b1850bf846",
   "metadata": {},
   "source": [
    "### Step 1 - A Quick EDA to know what we've got\n",
    "\n",
    "- use a basic entity mapping LLM call to create \"estimates\" for entities\n",
    "- This will allow me to roughly stratify a validation/test set\n",
    "- Plan is to do a brief annotation of a small number (20) to learn difficulties and figure out what EXACTLY I would want to see -> without this we are flying blind\n",
    "  - I considered using ontology databases to generate labels to validate on and then use the annotated as test but I thought that would be creating \"easy\" examples to validate from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdf67b4-1bb7-4dff-a50e-488ee85142be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# add the code for checking if we actually know what these concepts are\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m db = \u001b[43mclient\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mumls\u001b[39m\u001b[33m\"\u001b[39m]    \u001b[38;5;66;03m# Replace with your database name\u001b[39;00m\n\u001b[32m      4\u001b[39m collection = db[\u001b[33m\"\u001b[39m\u001b[33mmrconso\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m# Replace with your collection name\u001b[39;00m\n\u001b[32m      6\u001b[39m pipeline = [\n\u001b[32m      7\u001b[39m         {\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m$match\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m         }\n\u001b[32m     21\u001b[39m     ]\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# add the code for checking if we actually know what these concepts are\n",
    "\n",
    "db = client[\"umls\"]    # Replace with your database name\n",
    "collection = db[\"mrconso\"] # Replace with your collection name\n",
    "\n",
    "pipeline = [\n",
    "        {\n",
    "            \"$match\": {\n",
    "                \"STR_LOWER\": {\"$in\": lower_terms}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the default _id field\n",
    "                \"STR\": 1,  # Include STR field\n",
    "                \"SAB\": 1,  # Include SAB field\n",
    "                \"STR_LOWER\": 1,  # Include STR field\n",
    "                \"CODE\": 1  # Include CODE field\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Execute the aggregation pipeline and convert to DataFrame immediately\n",
    "aggregation_result_df = pd.DataFrame(list(collection.aggregate(pipeline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5309f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4abfc87d-e1f9-4312-ada2-484f689ac25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3032, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregation_result_df.shape\n",
    "# so we could fall back on this somehow\n",
    "\n",
    "# I will not though, I will use the annotated data and that's probably all I will have time for "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e82f78-c621-4779-b9e9-81f0402f2566",
   "metadata": {},
   "source": [
    "### Step 2) Process and prep annotated lables for modeling work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909034cf-a492-4de4-9fd3-4e950da1a660",
   "metadata": {},
   "source": [
    "#### Define Functions for annotation and premodeling and later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bd111d-35dd-47c3-8aa3-aa17af511947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions to use\n",
    "# The openapi calls here are for pre-modeling to help in annotation for just entities\n",
    "# and if you downloaded the repo you do not need to run unless you plan on annotating\n",
    "import json\n",
    "import pandas as pd\n",
    "run_annotation = False\n",
    "\n",
    "def create_entity_type_prompt(term_list):\n",
    "    '''Prompt generator for classifying entity types, with a regime flag.'''\n",
    "    \n",
    "    entity_dict = {\n",
    "        \"entities\": term_list\n",
    "    }\n",
    "    \n",
    "    entity_json_string = json.dumps(entity_dict, indent=2)\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are an expert in clinical informatics.\n",
    "    \n",
    "    Given a JSON list of concept names, return a JSON where each concept is assigned:\n",
    "    1. Its most appropriate **entity type(s)** from the following list:\n",
    "       - 'diagnosis'\n",
    "       - 'procedure'\n",
    "       - 'measurements/labs'\n",
    "       - 'medication'\n",
    "       - 'drug_class'\n",
    "       If more than one type applies, return them as a **comma-separated string** (e.g., \"medication,drug_class\").\n",
    "    \n",
    "    2. A boolean `is_regime` flag that is `true` if the term appears to refer to a multi-drug **regimen** or **combination therapy** (e.g., \"folfirinox\", \"FOLFOX\", \"triple therapy\") â€” otherwise, return `false`.\n",
    "    \n",
    "    Return the result in the following JSON format only:\n",
    "    ```json\n",
    "    {{\n",
    "      \"entities\": {{\n",
    "        \"[ENTITY_NAME]\": {{\n",
    "          \"entity_name\": \"[ENTITY_NAME]\",\n",
    "          \"types\": \"[ENTITY_TYPE]\",\n",
    "          \"is_regime\": true | false\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    Here is the input\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt + entity_json_string\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def flatten_entity_types_to_df(response_json):\n",
    "    '''Convert entity-type-only LLM output JSON to a DataFrame including regime flag.'''\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for entity, data in response_json.get(\"entities\", {}).items():\n",
    "        entity_name = data.get(\"entity_name\", entity)\n",
    "        types = data.get(\"types\", \"\")\n",
    "        is_regime = data.get(\"is_regime\", False)\n",
    "        \n",
    "        rows.append({\n",
    "            \"entity_name\": entity_name,\n",
    "            \"types\": types,\n",
    "            \"is_regime\": is_regime\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    return content\n",
    "\n",
    "\n",
    "def strip_markdown_fences(text):\n",
    "    # Remove triple backticks and optional \"json\" label around the JSON block\n",
    "    return re.sub(r\"^```json\\s*|```$\", \"\", text.strip(), flags=re.MULTILINE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff848117-0032-434a-a3d4-5c0f0ead5327",
   "metadata": {},
   "source": [
    "#### Run basic annotation LLM calls to allow for stratifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eba08df-e305-490a-a284-10bce31b2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "model=\"gpt-4-turbo\"\n",
    "\n",
    "# really only need to run this once\n",
    "if run_annotation == True:\n",
    "       \n",
    "    # this isn't relaly batching I'm just calling it that for right now\n",
    "    all_raw_results = []\n",
    "    for i in range(0, len(terms), batch_size):\n",
    "        batch_terms = terms[\"Concept Name\"].iloc[i:i+batch_size].tolist()\n",
    "        prompt = create_entity_type_prompt(batch_terms)\n",
    "        response_json = get_completion(prompt, model=model)  # get raw JSON/dict\n",
    "        all_raw_results.append(response_json)\n",
    "        \n",
    "    # stip any markdown and load to one large dataframe\n",
    "    df_full = pd.DataFrame()\n",
    "    for ent in all_raw_results:\n",
    "        ent_clean = json.loads(strip_markdown_fences(ent))\n",
    "        df_clean = flatten_entity_types_to_df(ent_clean)\n",
    "        df_full = pd.concat([df_full, df_clean])\n",
    "\n",
    "    assert len(set(df_full['entity_name'].to_list()) & set(terms['Concept Name'].to_list())), 'Assertion Error: Dataframes do not align\n",
    "    # keep it the same for storage\n",
    "    df_full = df_full.reset_index(drop=True)\n",
    "    \n",
    "    if not os.path.isfile('/pipeline_datalake/Annoted terms.xlsx'):\n",
    "        df_full.to_excel('/pipeline_datalake/Annoted terms.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe883d-870e-49cf-a916-cf5cf14e94b6",
   "metadata": {},
   "source": [
    "#### Take annotation results and create validation/test sets for annotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a13fb5-3e95-40b6-bcfe-6540b7c02d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_excel('/pipeline_datalake/Annoted terms.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f109a00-15de-470b-801a-3c3c82f09b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_use = df_full[df_full['types'].isin(['medication','diagnosis','measurements/labs','procedure'])]\n",
    "# Step 1: split off 60 samples (30 val + 30 test), stratified by 'types'\n",
    "df_26, df_output = train_test_split(\n",
    "    df_full_use,\n",
    "    train_size=26,\n",
    "    stratify=df_full_use['types'],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Step 2: split the 60 samples into 30 val and 30 test, stratified by 'types'\n",
    "df_val, df_test = train_test_split(\n",
    "    df_26,\n",
    "    train_size=13,\n",
    "    stratify=df_26['types'],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Validation set size: {len(df_val)}\")\n",
    "print(f\"Test set size: {len(df_test)}\")\n",
    "print(f\"Output set size: {len(df_output)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a0732-2cb8-4377-89a3-a920e16cc672",
   "metadata": {},
   "source": [
    "#### NOTE I later chose only 20 to be my validation set\n",
    "\n",
    "That's all I really had time for. Will annotate as many as I can for test but we'll do this in google docs\n",
    "\n",
    "Of course this assumes that the model did the entity assignments vaguely correctly but I can tweak if I need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dafccf-77c6-41b8-a763-2a80e2823bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[df_full['types'] == 'medication,drug_class'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b1d82-fa9b-4c8c-88e6-02730a0c5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[df_full['types'] == 'medication,drug_class'].tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c09a7-e03f-4ced-967c-e0cdbbd0deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.concat([df_val,df_full[df_full['types'] == 'medication,drug_class'].head(1) ])\n",
    "df_test = pd.concat([df_test,df_full[df_full['types'] == 'medication,drug_class'].tail(1) ])\n",
    "# and add the only it found as drug_class\n",
    "df_test = pd.concat([df_test,df_full[df_full['types'] == 'drug_class'].head(1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba0358-5289-4b70-b305-07ce6af3f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign\n",
    "df_val['set'] = 'val'\n",
    "df_test['set'] = 'test'\n",
    "df_output['set'] = 'output'\n",
    "final_labeled = pd.concat([df_val, df_test, df_output])\n",
    "\n",
    "assert final_labeled.shape[0] == terms['Concept Name'].nunique() == final_labeled['entity_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb660bc-e800-4049-97a9-74869f3101f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['types'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86857037-34ab-4f2e-85e7-60c504669422",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('/pipeline_datalake/Annoted terms With Set.xlsx'):\n",
    "        final_labeled.to_excel('/pipeline_datalake/Annoted terms With Set.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5648cb5-43a4-4919-aa17-e13b530ba03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ce4e397-001a-42a0-960d-97e21ad47454",
   "metadata": {},
   "source": [
    "### Step 3: Post-Annotation form evaluation dataframe for modeling \n",
    "Ready to go\n",
    "We continue in the next notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874f186d-a4e3-4380-8622-9b3b1abd5497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotated_df = pd.read_excel('/pipeline_datalake/Annoted terms With Labels.xlsx', sheet_name='entity_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2cd543-4548-4aed-a89b-61f53e524117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_df[annotated_df['validated'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94171133-15c5-458f-8275-c4b91131a205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_df[annotated_df['validated'] == 1]['entity_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79b9c3d-e7eb-4a7d-adc8-815d0ceef161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name</th>\n",
       "      <th>types</th>\n",
       "      <th>is_regime</th>\n",
       "      <th>set</th>\n",
       "      <th>should_say_no</th>\n",
       "      <th>codes_pipe</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>text</th>\n",
       "      <th>validated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liver Transplant Rejection</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T86.41</td>\n",
       "      <td>ICD-10</td>\n",
       "      <td>Liver transplant rejection</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oseltamivir</td>\n",
       "      <td>medication</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260101</td>\n",
       "      <td>RxNorm</td>\n",
       "      <td>oseltamivir</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lurbinectedin</td>\n",
       "      <td>medication</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2374729</td>\n",
       "      <td>RxNorm</td>\n",
       "      <td>lurbinectedin</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wheezing</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R06.2</td>\n",
       "      <td>ICD-10</td>\n",
       "      <td>Wheezing</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eptifibatide</td>\n",
       "      <td>medication</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75635</td>\n",
       "      <td>RxNorm</td>\n",
       "      <td>eptifibatide</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  entity_name       types  is_regime  set  should_say_no  \\\n",
       "0  Liver Transplant Rejection   diagnosis      False  val            NaN   \n",
       "1                 Oseltamivir  medication      False  val            NaN   \n",
       "2               Lurbinectedin  medication      False  val            NaN   \n",
       "3                    Wheezing   diagnosis      False  val            NaN   \n",
       "4                eptifibatide  medication      False  val            NaN   \n",
       "\n",
       "  codes_pipe vocabulary                        text  validated  \n",
       "0     T86.41     ICD-10  Liver transplant rejection        1.0  \n",
       "1     260101     RxNorm                 oseltamivir        1.0  \n",
       "2    2374729     RxNorm               lurbinectedin        1.0  \n",
       "3      R06.2     ICD-10                    Wheezing        1.0  \n",
       "4      75635     RxNorm                eptifibatide        1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3524289-cd68-42fe-9cdd-1706970f5887",
   "metadata": {},
   "source": [
    "#### Reflect on annotatoins\n",
    "\n",
    "6/20 were difficult\n",
    "\n",
    "green nails\t- diagnosis\n",
    "\n",
    "Dilation of hypoglossal nerve, open approach - \tprocedure\n",
    "\n",
    "Methylxanthine - \tmedication,drug_class\t\n",
    "\n",
    "long-acting beta agonist\t- drug_class\t\n",
    "\n",
    "Census Subregion - Mountain\t-  NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb40ad-b12c-46cf-b7c9-48820dc4c350",
   "metadata": {},
   "source": [
    "#### Can I make more samples automatically with a database?\n",
    "\n",
    "Prpbably not a good idea, I would have to keep that ratio of difficult terms balance\n",
    "I could use tools to lookup values if I have time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13 | packaged by conda-forge | (main, Jun  4 2025, 14:39:11) [GCC 13.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a46b7026314cc7a60511dcd81264ffd2e20560b39fc2da4adcba077a9f5495ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
